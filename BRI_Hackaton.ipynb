{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing & Inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "# selection and tunning\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_predict, KFold, train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "data_train  = pd.read_csv('train.csv')\n",
    "data_test   = pd.read_csv('test.csv')\n",
    "\n",
    "pd.set_option(\"max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[data_test.Education_level == 'level_5'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning & Inspecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify columns' names\n",
    "\n",
    "data_train.rename(columns = \n",
    "                  {'Employee_type'                        : 'employee_type',\n",
    "                  'marital_status_maried(Y/N)'            : 'marital_status', \n",
    "                  'number_of_dependences'                 : 'children',\n",
    "                  'Education_level'                       : 'education_level',\n",
    "                  'GPA'                                   : 'gpa',\n",
    "                  'assign_of_otherposition'               : 'job_otherposition',\n",
    "                  'annual leave'                          : 'annual_leave',\n",
    "                  'sick_leaves'                           : 'sick_leave',\n",
    "                  'Last_achievement_%'                    : 'last_achievement',\n",
    "                  'Achievement_above_100%_during3quartal' : 'achievement_above_100',\n",
    "                  'Best Performance'                      : 'performance'},\n",
    "                  inplace = True)\n",
    "\n",
    "data_test.rename(columns = \n",
    "                  {'Employee_type'                        : 'employee_type',\n",
    "                  'marital_status_maried(Y/N)'            : 'marital_status', \n",
    "                  'number_of_dependences'                 : 'children',\n",
    "                  'Education_level'                       : 'education_level',\n",
    "                  'GPA'                                   : 'gpa',\n",
    "                  'assign_of_otherposition'               : 'job_otherposition',\n",
    "                  'annual leave'                          : 'annual_leave',\n",
    "                  'sick_leaves'                           : 'sick_leave',\n",
    "                  'Last_achievement_%'                    : 'last_achievement',\n",
    "                  'Achievement_above_100%_during3quartal' : 'achievement_above_100',\n",
    "                  'Best Performance'                      : 'performance'},\n",
    "                  inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleansing\n",
    "\n",
    "def cleaning(data):\n",
    "    \n",
    "    # 'age' column descibes year of birth, turn it into age\n",
    "    \n",
    "    data['age'] = 2020 - data.age\n",
    "    \n",
    "    \n",
    "    # 'gpa' column hasn't right baseline on every education level, standarize:\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "    \n",
    "        edu = data.loc[i, 'education_level']\n",
    "        ipk = data.loc[i, 'gpa']\n",
    "        \n",
    "        if ((edu == 'level_0') or (edu == 'level_1')):\n",
    "        \n",
    "            if (ipk == 0 or ipk > 100) :\n",
    "                continue\n",
    "                \n",
    "            elif ipk <= 10:\n",
    "                data.loc[i, 'gpa'] = ipk / 10\n",
    "                \n",
    "            elif ipk <= 60:\n",
    "                 data.loc[i, 'gpa'] = ipk / 60\n",
    "            \n",
    "            else:\n",
    "                 data.loc[i, 'gpa'] = ipk / 100\n",
    "                \n",
    "\n",
    "        else:\n",
    "            \n",
    "            if (ipk == 0 or ipk > 400) :\n",
    "                continue\n",
    "                \n",
    "            elif ipk <= 4:\n",
    "                data.loc[i, 'gpa'] = ipk / 4\n",
    "            \n",
    "            elif ipk <= 40:        \n",
    "                data.loc[i, 'gpa'] = ipk / 40\n",
    "            \n",
    "            else:\n",
    "                data.loc[i, 'gpa'] = ipk / 400\n",
    "\n",
    "                \n",
    "cleaning(data_train)\n",
    "cleaning(data_test)\n",
    "\n",
    "## fill the outlier from level_1\n",
    "\n",
    "data_train.loc[108, 'gpa'] = np.median(data_train.gpa[data_train.education_level == 'level_1'])\n",
    "\n",
    "\n",
    "## drop null values in data_train\n",
    "\n",
    "data_train.dropna(inplace = True)\n",
    "data_train.reset_index(inplace = True)\n",
    "data_train.drop(columns = 'index', inplace=True)\n",
    "\n",
    "# concatenate train & test\n",
    "\n",
    "data_test['performance'] = np.NaN\n",
    "\n",
    "data_train['train_test'] = 1\n",
    "data_test['train_test']  = 0\n",
    "\n",
    "all_data = pd.concat([data_train, data_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate numerical & categorical data\n",
    "\n",
    "numerical_data    = data_train.select_dtypes(['int', 'float'])\n",
    "categorical_data  = data_train.select_dtypes(['object', 'bool'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in numerical_data.columns:\n",
    "    plt.hist(numerical_data[i], bins = 20)\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in numerical_data.columns:\n",
    "    sns.boxplot(numerical_data[i])\n",
    "    plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(numerical_data, index = 'performance', values = ['job_duration_in_current_job_level',\n",
    "       'job_duration_in_current_person_level',\n",
    "       'job_duration_in_current_branch', 'age', 'children', 'gpa',\n",
    "       'year_graduated', 'job_duration_from_training', 'branch_rotation',\n",
    "       'job_rotation', 'job_otherposition', 'annual_leave', 'sick_leave',\n",
    "       'last_achievement', 'achievement_above_100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(data_train.corr().performance).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categorical_data.columns:\n",
    "    sns.barplot(categorical_data[i].value_counts().index, \n",
    "                categorical_data[i].value_counts(),\n",
    "                ).set_title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# significancy checks on categorical data\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "categorical = data_train.select_dtypes(['object', 'bool'])\n",
    "\n",
    "\n",
    "for i in categorical.columns:\n",
    "    \n",
    "    pivoted = data_train.pivot_table(index   = 'performance',\n",
    "                                     columns = i,\n",
    "                                     values  = 'age',\n",
    "                                     aggfunc = np.count_nonzero,\n",
    "                                     fill_value=0)\n",
    "    \n",
    "    print(pivoted) \n",
    "    col = []\n",
    "    \n",
    "    for j in range(len(pivoted.columns)):\n",
    "        col.append(pivoted.columns[j])\n",
    "    \n",
    "    _, pval, _, _ = chi2_contingency(pivoted[col])\n",
    "    \n",
    "    print('\\n pval for {} is {} \\n\\n'.format(i, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering & selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize skewed distribution on numerical data\n",
    "\n",
    "all_data['norm_job_duration_in_current_branch'] = np.log(all_data.job_duration_in_current_branch+1)\n",
    "all_data['norm_age'] = np.log(all_data.age+1)\n",
    "all_data['norm_year_graduated'] = np.log(all_data.year_graduated+1)\n",
    "all_data['norm_job_duration_from_training'] = np.log(all_data.job_duration_from_training+1)\n",
    "all_data['norm_branch_rotation'] = np.log(all_data.branch_rotation+1)\n",
    "all_data['norm_job_rotation'] = np.log(all_data.job_rotation+1)\n",
    "all_data['norm_job_otherposition'] = np.log(all_data.job_otherposition+1)\n",
    "all_data['norm_annual_leave'] = np.log(all_data.annual_leave+1)\n",
    "all_data['norm_sick_leave'] = np.log(all_data.sick_leave+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recheck if the operation usefull\n",
    "\n",
    "abs(all_data[all_data.train_test == 1].corr().performance).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that have very low correlation\n",
    "\n",
    "all_data = all_data.drop(['norm_age', 'job_duration_in_current_branch', 'sick_leave', 'norm_branch_rotation',\n",
    "                          'norm_job_duration_from_training', 'annual_leave', 'norm_year_graduated',\n",
    "                          'year_graduated', 'job_rotation','job_otherposition',\n",
    "                          'job_duration_in_current_job_level', 'norm_job_rotation',\n",
    "                          'achievement_above_100'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.heatmap(abs(all_data.corr()), annot= True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummies for train data to check feature selection\n",
    "\n",
    "feat_sel = all_data[all_data.train_test == 1]\n",
    "\n",
    "X_feat_sel = pd.get_dummies(data = feat_sel, columns = feat_sel.select_dtypes(['object', 'bool']).columns)\n",
    "X_feat_sel = X_feat_sel.drop(columns = ['performance', 'train_test'])\n",
    "y_feat_sel = feat_sel.performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection insights\n",
    "\n",
    "models = [LogisticRegression(C=1, penalty='l2'), Lasso(alpha = 1.0)]\n",
    "\n",
    "for model in models:\n",
    "    rfe = RFE(model, 10)\n",
    "    fit = rfe.fit(X_feat_sel, y_feat_sel)\n",
    "    \n",
    "    print(\"Num Features: %s \\n\" % (fit.n_features_))\n",
    "    print(\"Selected Features: %s \\n\" % (fit.support_))\n",
    "    print(\"Feature Ranking: %s \\n \\n \\n \\n\" % (fit.ranking_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummies from OneHotEncoding\n",
    "\n",
    "all_data = pd.get_dummies(data = all_data, columns = all_data.select_dtypes(['object', 'bool']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the data\n",
    "\n",
    "all_data_cleaned = all_data[all_data.train_test == 1].drop(['train_test'], axis = 1)\n",
    "\n",
    "\n",
    "X_submission = X = all_data[all_data.train_test == 0].drop(['performance', 'train_test'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train - test\n",
    "data_train, data_test = train_test_split(all_data_cleaned, test_size = 0.2,\n",
    "                                         random_state = 10)\n",
    "\n",
    "\n",
    "# Define class target for testing\n",
    "X_test = data_test.drop(['performance'], axis = 1)\n",
    "y_test = data_test['performance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE\n",
    "X = data_train.drop(['performance'], axis = 1)\n",
    "y = data_train['performance']\n",
    "\n",
    "oversample = ADASYN()\n",
    "X_train, y_train = oversample.fit_resample(X, y)\n",
    "\n",
    "\n",
    "print(f'''shape of data before SMOTE: {X.shape}\n",
    "#shape of data after SMOTE: {X_train.shape}''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input classifiers here!\n",
    "\n",
    "classifiers = {}\n",
    "\n",
    "classifiers.update({\"AdaBoost\": AdaBoostClassifier()})\n",
    "classifiers.update({\"Gradient Boosting\": GradientBoostingClassifier()})\n",
    "classifiers.update({\"Bagging\": BaggingClassifier()})\n",
    "classifiers.update({\"Extra Trees Ensemble\": ExtraTreesClassifier()})\n",
    "classifiers.update({\"Random Forest\": RandomForestClassifier()})\n",
    "classifiers.update({\"XGB\": xgb.XGBRFClassifier(random_state=12345, nthread=-1)})\n",
    "\n",
    "\n",
    "# input parameters here!\n",
    "\n",
    "parameters = {}\n",
    "\n",
    "parameters.update({\"AdaBoost\": { \n",
    "                                \"classifier__base_estimator\": [DecisionTreeClassifier(max_depth = ii) for ii in range(1,6)],\n",
    "                                \"classifier__n_estimators\": [200],\n",
    "                                \"classifier__learning_rate\": [0.001, 0.01, 0.05, 0.1, 0.25, 0.50, 0.75, 1.0]\n",
    "                               }})\n",
    "\n",
    "parameters.update({\"Gradient Boosting\": { \n",
    "                                        \"classifier__learning_rate\":[0.15,0.1,0.05,0.01,0.005,0.001], \n",
    "                                        \"classifier__n_estimators\": [200],\n",
    "                                        \"classifier__max_depth\": [2,3,4,5,6],\n",
    "                                        \"classifier__min_samples_split\": [0.001, 0.01, 0.05, 0.10],\n",
    "                                        \"classifier__min_samples_leaf\": [0.001, 0.01, 0.05, 0.10],\n",
    "                                        \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                                        \"classifier__subsample\": [1]\n",
    "                                         }})\n",
    "\n",
    "parameters.update({\"Bagging\": { \n",
    "                                \"classifier__base_estimator\": [DecisionTreeClassifier(max_depth = ii) for ii in range(1,6)],\n",
    "                                \"classifier__n_estimators\": [200],\n",
    "                                \"classifier__max_features\": [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                                \"classifier__n_jobs\": [-1]\n",
    "                                }})\n",
    "\n",
    "parameters.update({\"Extra Trees Ensemble\": { \n",
    "                                            \"classifier__n_estimators\": [200],\n",
    "                                            \"classifier__class_weight\": [None, \"balanced\"],\n",
    "                                            \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                                            \"classifier__max_depth\" : [3, 4, 5, 6, 7, 8],\n",
    "                                            \"classifier__min_samples_split\": [0.001, 0.01, 0.05, 0.10],\n",
    "                                            \"classifier__min_samples_leaf\": [0.001, 0.01, 0.05, 0.10],\n",
    "                                            \"classifier__criterion\" :[\"gini\", \"entropy\"]     ,\n",
    "                                            \"classifier__n_jobs\": [-1]\n",
    "                                             }})\n",
    "\n",
    "parameters.update({\"Random Forest\": { \n",
    "                                    \"classifier__n_estimators\": [200],\n",
    "                                    \"classifier__class_weight\": [None, \"balanced\"],\n",
    "                                    \"classifier__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "                                    \"classifier__max_depth\" : [3, 4, 5, 6, 7, 8],\n",
    "                                    \"classifier__min_samples_split\": [0.001, 0.01, 0.05, 0.10],\n",
    "                                    \"classifier__min_samples_leaf\": [0.001, 0.01, 0.05, 0.10],\n",
    "                                    \"classifier__criterion\" :[\"gini\", \"entropy\"]     ,\n",
    "                                    \"classifier__n_jobs\": [-1]\n",
    "                                     }})\n",
    "\n",
    "parameters.update({\"XGB\": { \n",
    "                           'classifier__min_child_weight': [1, 5, 10],\n",
    "                           'classifier__gamma': [0.5, 1, 1.5, 2, 5],\n",
    "                           'classifier__subsample': [0.6, 0.8, 1.0],\n",
    "                           'classifier__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                           'classifier__max_depth': [3, 4, 5, 10, 12, 15],\n",
    "                           'classifier__learning_rate' : [0.01, 0.02, 0.1, 0.25, 0.5],\n",
    "                           }})\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "\n",
    "# Tune and evaluate classifiers\n",
    "for classifier_label, classifier in classifiers.items():\n",
    "    # Print message to user\n",
    "    print(\"Now tuning {} :\".format(classifier_label))\n",
    "    \n",
    "    # Scale features via Z-score normalization\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Define steps in pipeline\n",
    "    steps = [(\"scaler\", scaler), (\"classifier\", classifier)]\n",
    "    \n",
    "    # Initialize Pipeline object\n",
    "    pipeline = Pipeline(steps = steps)\n",
    "      \n",
    "    # Define parameter grid\n",
    "    param_grid = parameters[classifier_label]\n",
    "    \n",
    "    # Initialize GridSearch object\n",
    "    gscv = GridSearchCV(pipeline, param_grid, cv = 5,  n_jobs= -1, verbose = 1, scoring = \"accuracy\", return_train_score=True)\n",
    "                      \n",
    "    # Fit gscv\n",
    "    gscv.fit(X_train, y_train)  \n",
    "    \n",
    "    # Get best parameters and score\n",
    "    best_params = gscv.best_params_\n",
    "    best_score = gscv.best_score_\n",
    "    \n",
    "    # Update classifier parameters and define new pipeline with tuned classifier\n",
    "    tuned_params = {item[12:]: best_params[item] for item in best_params}\n",
    "    classifier.set_params(**tuned_params)\n",
    "            \n",
    "    # Make predictions\n",
    "    y_pred = gscv.predict(X_test)\n",
    "        \n",
    "    # Get AUC\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    # Get f1 score\n",
    "    y_pred = gscv.predict(X_test)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Get recall score\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "\n",
    "    # Get precision score\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    \n",
    "    # Get accuracy score\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # False Positive rate\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    fp_rate = (fp)/(fp + tn)\n",
    "    \n",
    "    plot_confusion_matrix(gscv, X_test, y_test) \n",
    "    \n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.show()\n",
    "    \n",
    "    test_scores = gscv.cv_results_['mean_test_score']\n",
    "    train_scores = gscv.cv_results_['mean_train_score'] \n",
    "\n",
    "    plt.plot(test_scores, label='test')\n",
    "    plt.plot(train_scores, label='train')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    # Save results\n",
    "    result = {\"Classifier\": classifier_label,\n",
    "              \"Best Parameters\": best_params,\n",
    "              \"Training Recall Score\": best_score,\n",
    "              \"Test Recall Score\": recall,\n",
    "              \"Test Precision Score\": precision,\n",
    "              \"Test Accuracy Score\" : accuracy,\n",
    "              \"Test F1 Score\": f1,\n",
    "              \"False Positive Rate\": fp_rate,\n",
    "              \"Test AUC\": auc}\n",
    "    \n",
    "    results.update({classifier_label: result})  \n",
    "    \n",
    "    print(f'''Best Parameters       : {best_params} \\n,\n",
    "          Training Recall Score : {best_score} \\n,\n",
    "          Test Recall Score     : {recall} \\n,\n",
    "          Test Precision Score  : {precision} \\n,\n",
    "          Test Accuracy Score   : {accuracy} \\n,\n",
    "          Test F1 Score         : {f1} \\n,\n",
    "          False Positive Rate   : {fp_rate} \\n,\n",
    "          Test AUC              : {auc}''')\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submission = gscv.predict_proba(X_submission)\n",
    "submission = pd.DataFrame(y_submission)\n",
    "submission.rename(columns = {1 : 'Best Performance'}, inplace = True)\n",
    "submission = submission['Best Performance']\n",
    "submission = submission.reset_index()\n",
    "\n",
    "submission.to_csv(f'''submission_XGB.csv''', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
